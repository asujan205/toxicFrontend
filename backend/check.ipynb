{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Read the dataset from a CSV file\n",
    "df = pd.read_csv(os.path.join('data', 'final.csv'))\n",
    "# df = df.sample(frac=1, random_state=random.seed())\n",
    "# Specify the column name where commas need to be removed\n",
    "column_name = 'comment_text'\n",
    "columns_to_convert = ['toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "# Remove commas from the specified column\n",
    "df[column_name] = df[column_name].str.replace(',', '').str.lower()\n",
    "# df[column_name] = df[column_name].apply(lambda x: x.astype(str).str.lower())\n",
    "df[columns_to_convert] = df[columns_to_convert].fillna(0).astype(int)\n",
    "\n",
    "# Save the modified dataset to a new CSV file\n",
    "df.to_csv('updated_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "\n",
    "df = pd.read_csv(os.path.join('data', 'final.csv'))\n",
    "# Convert the text data into a TensorFlow dataset\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(df['comment_text'])\n",
    "\n",
    "# Define the data augmentation transformations\n",
    "def apply_augmentation(text):\n",
    "    # Apply data augmentation operations\n",
    "    # Add your own data augmentation logic here\n",
    "    augmented_text = text  # Placeholder logic; replace with actual data augmentation\n",
    "    return augmented_text\n",
    "\n",
    "# Apply data augmentation to the text dataset and save to a new CSV file\n",
    "augmented_data = []\n",
    "for text in text_dataset:\n",
    "    augmented_text = apply_augmentation(text)\n",
    "    augmented_data.append(augmented_text.numpy().decode('utf-8'))\n",
    "\n",
    "augmented_df = pd.DataFrame({'comment_text': augmented_data})\n",
    "augmented_df.to_csv('output.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "comment='fuck you shit asshole'\n",
    "\n",
    "# Tokenize the comment and create a vocabulary\n",
    "word_counts = {}\n",
    "words = comment.lower().split()\n",
    "for word in words:\n",
    "  if word not in word_counts:\n",
    "      word_counts[word] = 1\n",
    "  else:\n",
    "      word_counts[word] += 1\n",
    "\n",
    "# Sort the words by frequency and keep the most frequent ones\n",
    "sorted_words = sorted(word_counts, key=lambda x: word_counts[x], reverse=True)\n",
    "sorted_words = sorted_words[:200000]\n",
    "\n",
    "# Create a word-to-index mapping\n",
    "word_to_index = {word: index + 1 for index, word in enumerate(sorted_words)}\n",
    "\n",
    "# Convert the comment to a sequence of integers\n",
    "sequence = []\n",
    "for word in words:\n",
    "  if word in word_to_index:\n",
    "      sequence.append(word_to_index[word])\n",
    "\n",
    "  # Pad the sequence to a fixed length\n",
    "if len(sequence) > 300:\n",
    "    sequence = sequence[:300]\n",
    "else:\n",
    "    sequence = sequence + [0] * (300 - len(sequence))\n",
    "\n",
    "  # Convert to a numpy array\n",
    "vectorized_comment = np.array([sequence])\n",
    " \n",
    "vectorized_comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
